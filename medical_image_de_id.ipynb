{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-identify medical images with the help of Amazon Comprehend Medical and Rekognition\n",
    "\n",
    "Medical images are a foundational tool in modern medicine that enable clinicians to visualize critical information about a patient to help diagnose and treat them. The digitization of medical images has vastly improved our ability to reliably store, share, view, search, and curate these images to assist our medical professionals. The number of modalities for medical images has also increased. From CT scans to MRIs, digital pathology to ultrasounds, there are vast amounts of medical data collected in medical image archives.\n",
    "\n",
    "These medical images are also useful for medical research. Using machine learning, our global medical research institutions are now pressing forward into insights that can only be gained by analyzing hundreds of thousands or millions of images. Accomplishing this while complying with regulatory obligations like the Health Information Portability and Accountability Act (HIPAA) can be challenging for medical professionals. Often, these medical images contain Protected Health Information (PHI) stored as text within the image itself. This has historically presented a challenge because removing this PHI required manual review and editing of the image. This manual process can easily take many minutes per image and makes de-identifying large datasets time consuming and expensive.\n",
    "\n",
    "Last year, Amazon announced the ability to easily detect and extract text from images using our machine learning service [Amazon Rekognition](https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-rekognition-announces-real-time-face-recognition-text-in-image-recognition-and-improved-face-detection/ \"Amazon Rekognition\"). This year, we announced a new machine learning Natural Language Processing (NLP) service for medical text called [Amazon Comprehend Medical](https://aws.amazon.com/about-aws/whats-new/2018/11/introducing-amazon-comprehend-medical/) that can help customers to detect and identify PHI in a string of text. Combining these two services together with some Python code, as demonstrated below, can help customers inexpensively and quickly detect, identify, and redact PHI from within medical images.\n",
    "\n",
    "\n",
    "## De-identification architecture\n",
    "\n",
    "In this example, we will use the Notebooks feature of Amazon SageMaker to create an interactive notebook with Python code. These notebooks are just one part of Amazon SageMaker, a fully-managed service that covers the entire machine learning workflow to label and prepare your data, choose an algorithm, train the algorithm, tune and optimize it for deployment, make predictions, and take action. In this example though, for the actual machine learning and prediction, we will be using Amazon Rekognition to extract text from the images and Amazon Comprehend Medical to help us to identify and detect the PHI. All of our image files will be read from and written to a bucket in Amazon Simple Storage Service (Amazon S3), an object storage service that offers industry-leading scalability, data availability, security, and performance.\n",
    "\n",
    "![alt-text](de-identify-medical-images-blog-post.jpg \"diagram\")\n",
    "\n",
    "When using Amazon Comprehend Medical to detect and identify protected health information, please note that the service provides confidence scores for each identified entity that indicates the level of confidence in the accuracy of the detected entity. You should take these confidence scores into account and review identified entities to make sure they are correct and appropriate for your use case.  For more information about confidence scores, please see the [Amazon Comprehend Medical documentation](https://docs.aws.amazon.com/comprehend/latest/dg/how-medical-phi.html).\n",
    "\n",
    "\n",
    "## Using the notebook\n",
    "\n",
    "This notebook shows an example chest x-ray image from a dataset made available by the NIH Clinical Center.  The dataset can be downloaded from [this link](https://nihcc.app.box.com/v/ChestXray-NIHCC).  Please see the NIH Clinical Center’s [CVPR 2017 paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf) for more information.\n",
    "\n",
    "At the very beginning of the notebook, you will see 5 parameters you can adjust to control the de-identification process outlined in this example.\n",
    "  \n",
    "* **bucket** defines the S3 bucket where images will be read from and written to.\n",
    "* **object** defines the identified image that you want to de-identify. These can be PNG, JPG, or DICOM images.  If the object ends with the extension ‘.dcm’, then the image is assumed to be a DICOM image and the ImageMagick utility will be used to convert it to PNG format before processing it.\n",
    "* **redacted_box_color** defines the color that will be used to cover up identified PHI text within the image.\n",
    "* **dpi** defines the dpi setting that will be used in the output image\n",
    "* **phi_detection_threshold** is the threshold for the confidence score mentioned above (between 0.00 and 1.00). Text detected and identified by Amazon Comprehend Medical must meet the minimum confidence score you set to be redacted from the output image. The default value of 0.00 will redact all text that Amazon Comprehend Medical has detected an identified as PHI, regardless of the confidence score.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DICOM to PNG conversion function\n",
    "import sys\n",
    "!{sys.executable} -m pip --disable-pip-version-check install pypng pydicom numpy\n",
    "import png\n",
    "import pydicom\n",
    "import numpy as np\n",
    "\n",
    "def dicom_to_png(dcmfile, pngfile):\n",
    "\n",
    "    ds = pydicom.dcmread(dcmfile)\n",
    "    shape = ds.pixel_array.shape\n",
    "    # Convert to float to avoid overflow or underflow losses.\n",
    "    image_2d = ds.pixel_array.astype(float)\n",
    "    # Rescaling grey scale between 0-255\n",
    "    image_2d_scaled = (np.maximum(image_2d,0) / image_2d.max()) * 255.0\n",
    "    image_2d_scaled = np.uint8(image_2d_scaled)\n",
    "    with open(pngfile, 'wb') as png_file:\n",
    "        w = png.Writer(shape[1], shape[0], greyscale=True)\n",
    "        w.write(png_file, image_2d_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the S3 bucket and object for the medical image we want to analyze.  Also define the color used for redaction.\n",
    "bucket = 'aws-ml-blog'\n",
    "object = 'artifacts/de-id-medical-images/test.png'\n",
    "\n",
    "redacted_box_color = 'red'\n",
    "dpi = 72\n",
    "phi_detection_threshold = 0.50\n",
    "\n",
    "#Import all of the required libraries\n",
    "%matplotlib inline\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from os.path import basename, splitext\n",
    "\n",
    "#Implement AWS Services\n",
    "rekognition = boto3.client('rekognition')\n",
    "comprehendmedical = boto3.client(service_name='comprehendmedical')\n",
    "s3 = boto3.resource('s3')\n",
    "img_bucket = s3.Bucket(bucket)\n",
    "\n",
    "# Convert DICOM to PNG if necessary\n",
    "if (splitext(object)[1]) == '.dcm':\n",
    "    imagefile = basename(object)\n",
    "    img_bucket.download_file(object, imagefile)\n",
    "    object = object + '.png'\n",
    "    pngfile = basename(object)\n",
    "    dicom_to_png(imagefile, pngfile)\n",
    "    img_bucket.upload_file(pngfile, object)\n",
    "\n",
    "#Download the image from S3 and hold it in memory\n",
    "img_object = img_bucket.Object(object)\n",
    "xray = io.BytesIO()\n",
    "img_object.download_fileobj(xray)\n",
    "img = np.array(Image.open(xray), dtype=np.uint8)\n",
    "\n",
    "#Set the image color map to grayscale, turn off axis graphing, and display the image\n",
    "height, width = img.shape[0:2]\n",
    "# What size does the figure need to be in inches to fit the image?\n",
    "figsize = width / float(dpi), height / float(dpi)\n",
    "# Create a figure of the right size with one axes that takes up the full figure\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "# Hide spines, ticks, etc.\n",
    "ax.axis('off')\n",
    "# Display the image.\n",
    "ax.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Amazon Rekognition to detect all of the text in the medical image\n",
    "#response=rekognition.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':object}})\n",
    "response=rekognition.detect_text(Image={'Bytes':xray.getvalue()})\n",
    "textDetections=response['TextDetections']\n",
    "print('Aggregating detected text...')\n",
    "textblock=\"\"\n",
    "offsetarray=[]\n",
    "totallength=0\n",
    "\n",
    "#The various text detections are returned in a JSON object.  Aggregate the text into a single large block and\n",
    "#keep track of the offsets.  This will allow us to make a single call to Amazon Comprehend Medical for\n",
    "#PHI detection and minimize our Comprehend Medical service charges.\n",
    "for text in textDetections:\n",
    "    if text['Type'] == \"LINE\":\n",
    "            offsetarray.append(totallength)\n",
    "            totallength+=len(text['DetectedText'])+1\n",
    "            textblock=textblock+text['DetectedText']+\" \"  \n",
    "            print(f\"adding '{text['DetectedText']}', length: {len(text['DetectedText'])}, offsetarray: {offsetarray}\")\n",
    "offsetarray.append(totallength)\n",
    "totaloffsets=len(offsetarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call Amazon Comprehend Medical and pass it the aggregated text from our medical image.\n",
    "phi_boxes_list=[]\n",
    "if len(textblock) > 0:\n",
    "    philist=comprehendmedical.detect_phi(Text = textblock)\n",
    "else:\n",
    "    philist={'Entities': []}\n",
    "\n",
    "#Amazon Comprehend Medical will return a JSON object that contains all of the PHI detected in the text block with\n",
    "#offset values that describe where the PHI begins and ends.  We can use this to determine which of the text blocks \n",
    "#detected by Amazon Rekognition should be redacted.  The 'phi_boxes_list' list is created to keep track of the\n",
    "#bounding boxes that potentially contain PHI.\n",
    "print('Finding PHI text...')\n",
    "not_redacted=0\n",
    "for phi in philist['Entities']:\n",
    "    if phi['Score'] > phi_detection_threshold:\n",
    "        for i in range(0,totaloffsets-1):\n",
    "            if offsetarray[i] <= phi['BeginOffset'] < offsetarray[i+1]:\n",
    "                if textDetections[i]['Geometry']['BoundingBox'] not in phi_boxes_list:\n",
    "                    print(f\"'{phi['Text']}' was detected as type '{phi['Type']}' and will be redacted.\")\n",
    "                    phi_boxes_list.append(textDetections[i]['Geometry']['BoundingBox'])\n",
    "    else:\n",
    "        print(f\"'{phi['Text']}' was detected as type '{phi['Type']}', but did not meet the confidence score threshold and will not be redacted.\")\n",
    "        not_redacted+=1\n",
    "print(f\"Found {len(phi_boxes_list)} text boxes to redact.\")\n",
    "print(f\"{not_redacted} additional text boxes were detected, but did not meet the confidence score threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now this list of bounding boxes will be used to draw red boxes over the PHI text.\n",
    "height, width = img.shape[0:2]\n",
    "# What size does the figure need to be in inches to fit the image?\n",
    "figsize = width / float(dpi), height / float(dpi)\n",
    "# Create a figure of the right size with one axes that takes up the full figure\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.imshow(img)\n",
    "plt.imshow(img, cmap='gray')\n",
    "for box in phi_boxes_list:\n",
    "    #The bounding boxes are described as a ratio of the overall image dimensions, so we must multiply them\n",
    "    #by the total image dimensions to get the exact pixel values for each dimension.\n",
    "    x = img.shape[1] * box['Left']\n",
    "    y = img.shape[0] * box['Top']\n",
    "    width = img.shape[1] * box['Width']\n",
    "    height = img.shape[0] * box['Height']\n",
    "    rect = patches.Rectangle((x,y),width,height,linewidth=0,edgecolor=redacted_box_color,facecolor=redacted_box_color)\n",
    "    ax.add_patch(rect)\n",
    "#Ensure that no axis or whitespaces is printed in the image file we want to save.\n",
    "plt.axis('off')    \n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "#Save redacted medical image to the same Amazon S3 bucket, in PNG format, with 'de-id-' in front of the original\n",
    "#filename.\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, bbox_inches='tight', pad_inches=0, format='png')\n",
    "img_data.seek(0)\n",
    "#Uncomment the line below to write the redacted image to S3\n",
    "#img_bucket.put_object(Body=img_data, ContentType='image/png', Key='de-id-'+object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}